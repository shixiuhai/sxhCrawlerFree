import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time
from collections import deque
from gne import GeneralNewsExtractor
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

def news_crawler(start_url, delay=1, max_pages=50, page_callback=None, thread_count=5):
    """
    é«˜æ ¡æ–°é—»ç½‘ç«™ä¸“ç”¨çˆ¬è™«ï¼ˆçº¿ç¨‹æ± ç®€åŒ–ç‰ˆï¼‰
    """
    domain = urlparse(start_url).netloc
    visited = set()
    queue = deque([start_url])
    
    print(f"å¼€å§‹çˆ¬å–é«˜æ ¡æ–°é—»ç½‘ç«™: {start_url} (ä½¿ç”¨{thread_count}ä¸ªçº¿ç¨‹)")
    start_time = time.time()
    
    def process_url(url):
        """å¤„ç†å•ä¸ªURLçš„å‡½æ•°"""
        try:
            time.sleep(delay)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç¼–ç å¤„ç†
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            if not response.encoding or response.encoding.lower() not in ['utf-8', 'utf8']:
                response.encoding = response.apparent_encoding or 'utf-8'
                
            if 'text/html' in response.headers.get('Content-Type', ''):
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if page_callback:
                    page_callback(url, response.text)
                
                # è§£æé“¾æ¥
                soup = BeautifulSoup(response.content, 'html.parser')
                new_links = []
                for link in soup.find_all('a', href=True):
                    absolute_url = urljoin(url, link['href'])
                    
                    if (absolute_url.startswith(('http://', 'https://')) and
                        urlparse(absolute_url).netloc == domain and
                        absolute_url not in visited):
                        new_links.append(absolute_url)
                
                # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—
                for link in new_links:
                    if link not in visited and link not in queue:
                        queue.append(link)
            
            # æ ‡è®°ä¸ºå·²è®¿é—®
            visited.add(url)
            
            print(f"å·²è®¿é—®: {url} ({len(visited)}/{max_pages})")
            return True
            
        except Exception as e:
            print(f"çˆ¬å– {url} æ—¶å‡ºé”™: {str(e)}")
            visited.add(url)
            return False
    
    # ä½¿ç”¨çº¿ç¨‹æ± 
    with ThreadPoolExecutor(max_workers=thread_count) as executor:
        while len(visited) < max_pages and queue:
            # è·å–å½“å‰éœ€è¦å¤„ç†çš„URL
            current_urls = []
            while queue and len(current_urls) < thread_count and len(visited) + len(current_urls) < max_pages:
                url = queue.popleft()
                if url not in visited:
                    current_urls.append(url)
            
            if not current_urls:
                break
            
            # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            futures = []
            for url in current_urls:
                futures.append(executor.submit(process_url, url))
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"çº¿ç¨‹æ‰§è¡Œå‡ºé”™: {str(e)}")
    
    end_time = time.time()
    print(f"\nçˆ¬å–å®Œæˆ! æ€»å…±è®¿é—®äº† {len(visited)} ä¸ªé¡µé¢")
    print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")

# åˆå§‹åŒ–æ–°é—»æå–å™¨
extractor = GeneralNewsExtractor()

def handle_news_page(url, html_content):
    """
    ç®€å•çš„æ–°é—»å¤„ç†å›è°ƒå‡½æ•°
    """
    try:
        news_result = extractor.extract(html_content)
        
        if news_result and news_result.get('title') and news_result.get('content'):
            news_result['url'] = url
            news_result['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            news_content = news_result.get('content', '')
            
            print(f"\nğŸ“° å‘ç°æ–°é—»:")
            print(f"   æ ‡é¢˜: {news_result.get('title')}")
            print(f"   æ—¶é—´: {news_result.get('publish_time', 'æœªçŸ¥')}")
            print(f"   ä½œè€…: {news_result.get('author', 'æœªçŸ¥')}")
            print(f"   ç½‘å€: {news_result.get('url')}")
            
    except Exception:
        pass

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    news_crawler(
        start_url="https://www.tsinghua.edu.cn/info/1177/121162.htm",
        delay=0.5,
        max_pages=30,
        page_callback=handle_news_page,
        thread_count=3
    )