import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time
from collections import deque
from gne import GeneralNewsExtractor
from datetime import datetime

def news_crawler(start_url, delay=1, max_pages=50, page_callback=None):
    """
    é«˜æ ¡æ–°é—»ç½‘ç«™ä¸“ç”¨çˆ¬è™«
    
    å‚æ•°:
        start_url: èµ·å§‹URL
        delay: è¯·æ±‚å»¶è¿Ÿ(ç§’)
        max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
        page_callback: é¡µé¢å¤„ç†å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(url, response_text)
    """
    domain = urlparse(start_url).netloc
    visited = set()
    queue = deque([start_url])
    
    print(f"å¼€å§‹çˆ¬å–é«˜æ ¡æ–°é—»ç½‘ç«™: {start_url}")
    start_time = time.time()
    
    while queue and len(visited) < max_pages:
        url = queue.popleft()
        
        if url in visited:
            continue
            
        try:
            time.sleep(delay)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            # æ‰‹åŠ¨è®¾ç½®ç¼–ç ï¼ˆå¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼‰
            if not response.encoding or response.encoding.lower() not in ['utf-8', 'utf8']:
                response.encoding = response.apparent_encoding or 'utf-8'
                
            
            if 'text/html' in response.headers.get('Content-Type', ''):
                # å¦‚æœæœ‰é¡µé¢å›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                if page_callback:
                    page_callback(url, response.text)
                
                # è§£æHTMLæŸ¥æ‰¾é“¾æ¥
                soup = BeautifulSoup(response.content, 'html.parser')
                for link in soup.find_all('a', href=True):
                    absolute_url = urljoin(url, link['href'])
                    
                    if (absolute_url.startswith(('http://', 'https://')) and
                        urlparse(absolute_url).netloc == domain and
                        absolute_url not in visited and
                        absolute_url not in queue):
                        
                        queue.append(absolute_url)
            
            visited.add(url)
            print(f"å·²è®¿é—®: {url} ({len(visited)}/{max_pages})")
            
        except Exception as e:
            print(f"çˆ¬å– {url} æ—¶å‡ºé”™: {str(e)}")
            visited.add(url)
    
    end_time = time.time()
    print(f"\nçˆ¬å–å®Œæˆ! æ€»å…±è®¿é—®äº† {len(visited)} ä¸ªé¡µé¢")
    print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")

# åˆå§‹åŒ–æ–°é—»æå–å™¨ï¼ˆå…¨å±€ä¸€ä¸ªå°±å¤Ÿäº†ï¼‰
extractor = GeneralNewsExtractor()

def handle_news_page(url, html_content):
    """
    ç®€å•çš„æ–°é—»å¤„ç†å›è°ƒå‡½æ•°
    æ¯å‘ç°ä¸€ä¸ªé¡µé¢å°±è°ƒç”¨è¿™ä¸ªå‡½æ•°
    """
    try:
        # ä½¿ç”¨GNEæå–æ–°é—»å†…å®¹
        news_result = extractor.extract(html_content)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°æ–°é—»
        if news_result and news_result.get('title') and news_result.get('content'):
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            news_result['url'] = url
            news_result['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            news_content = news_result.get('content', '')
            
            # æ‰“å°æ–°é—»ä¿¡æ¯
            print(f"\nğŸ“° å‘ç°æ–°é—»:")
            print(f"   æ ‡é¢˜: {news_result.get('title')}")
            print(f"   æ—¶é—´: {news_result.get('publish_time', 'æœªçŸ¥')}")
            print(f"   ä½œè€…: {news_result.get('author', 'æœªçŸ¥')}")
            print(f"   ç½‘å€: {news_result.get('url')}")
            # print(f"   å†…å®¹é¢„è§ˆ: {news_content[:100]}...")  # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            print(f"   å†…å®¹é¢„è§ˆ: {news_content}")  # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä½ çš„å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚ï¼š
            # 1. å­˜å…¥æ•°æ®åº“
            # 2. å‘é€åˆ°å…¶ä»–ç³»ç»Ÿ
            # 3. å®æ—¶åˆ†æå†…å®¹
            
    except Exception as e:
        # å¦‚æœä¸æ˜¯æ–°é—»é¡µé¢ï¼Œé™é»˜è·³è¿‡
        pass

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç›´æ¥ä½¿ç”¨ç®€å•çš„å›è°ƒå‡½æ•°
    news_crawler(
        start_url="https://www.tsinghua.edu.cn/info/1177/118220.htm",  # æ›¿æ¢æˆä½ è¦çˆ¬çš„å­¦æ ¡æ–°é—»ç½‘
        delay=1,
        max_pages=20,
        page_callback=handle_news_page  # ç›´æ¥ä¼ å…¥å›è°ƒå‡½æ•°
    )